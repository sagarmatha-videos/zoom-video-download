{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automatically downlod links not downloaded**<br>\n",
    "**or file that doesn't end with .mp4 extension**\n",
    "colab: https://colab.research.google.com/drive/1TCYgn7MPdptT_aDChtysmSuvhoiVb0nB#scrollTo=ZBwmLxMAISTd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installations\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# updating nodejs from v14 -> v16 :: reference : https://www.digitalocean.com/community/tutorials/how-to-install-node-js-on-debian-10\n",
    "# !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -\n",
    "# !sudo apt install nodejs\n",
    "# !node -v\n",
    "\n",
    "!apt-get update && upgrade\n",
    "!apt install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "!npm i convert-excel-to-json\n",
    "!npm install puppeteer\n",
    "!npm install puppeteer-extra\n",
    "!npm install puppeteer-extra --save\n",
    "!npm install puppeteer-extra-plugin-stealth --save   # installs plugin\n",
    "!npm install puppeteer-extra-plugin-recaptcha\n",
    "# !npm install -g npm to update # update npm\n",
    "# !npm install robots-txt-parser --save\n",
    "# !npm install random-int"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save links to json file cause it was hard to split link['E'], was converted to UTC and was showing err. link.E.getFullYear is not a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of all links to .json format\n",
    "# %%writefile ./drive/MyDrive/zoom_downloads/not_downloaded.js\n",
    "%%writefile ./test.js\n",
    "// check file that does not exist from links\n",
    "// may miss two classes same day\n",
    "\n",
    "var fs = require('fs');\n",
    "const excelToJson = require('convert-excel-to-json');\n",
    "const path = require('path');\n",
    "const download_root = './drive/MyDrive/zoom_downloads/'\n",
    "\n",
    "// function to save_json_data\n",
    "let save_json_data = function (data, file_path){\n",
    "  let to_save_data  = JSON.stringify(data);\n",
    "try {\n",
    "    fs.writeFileSync(file_path, to_save_data);\n",
    "} catch (error) {\n",
    "    console.log('Error saving data to file error_data...' + file_path + error);\n",
    "}\n",
    "}\n",
    "// function to load json data :: to test if data is actually saved\n",
    "let load_json_data = (file_path) => {\n",
    "  // loading error links\n",
    "  // var saved_data;\n",
    "  try {\n",
    "      var saved_data = fs.readFileSync(file_path, 'utf-8');\n",
    "      saved_data = JSON.parse(saved_data);\n",
    "      // console.log('Loaded Links...: \\n ' + saved_data);\n",
    "      return saved_data;\n",
    "  } catch (error) {\n",
    "      console.log('Error Loading json file ...: \\n ' + file_path + error); \n",
    "  }\n",
    "}\n",
    "var links_to_download = []\n",
    "const all_links = excelToJson({\n",
    "    sourceFile: download_root + 'Fuse-Links.xlsx'\n",
    "})['sagarmatha_live_class'];\n",
    "\n",
    "save_json_data(all_links, download_root + 'links.json')\n",
    "\n",
    "\n",
    "all_links_saved = load_json_data(download_root + 'links.json')\n",
    "console.log('all_links_saved', all_links_saved)\n",
    "if (all_links_saved) {\n",
    "  console.log('---------------------------------------')\n",
    "  console.log('successfully saved all links')\n",
    "  console.log('---------------------------------------')\n",
    "  }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!node test.js"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(python code)<br>\n",
    "saving link_indexes yet to be download at file: ./drive/MyDrive/zoom_downloads/progress_logs_v3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "download_root = './drive/MyDrive/zoom_downloads/'\n",
    "os.path.exists(download_root)\n",
    "\n",
    "\n",
    "toDownload = []\n",
    "data = []\n",
    "with open(download_root + 'links.json','r') as file:\n",
    "  links = json.load(file)\n",
    "links = links[:]\n",
    "for index,link in enumerate(links[1:]):\n",
    "      if index == 1: continue  # pass first row (title row)\n",
    "      \n",
    "      link['B'] = link['B'].replace('/','|')\n",
    "      link['C'] = link['C'].replace('/','|')\n",
    "      link['G'] = link['G'].replace('/','|')\n",
    "      corresponding_folder_name = link['B'] + '[' + link['C'] + '][' + link['G'] + ']'\n",
    "\n",
    "      # file doesnot exists so need to downloads\n",
    "      if not os.path.exists(download_root + corresponding_folder_name):\n",
    "        print(f'folder doesn\\'t exist {corresponding_folder_name}')\n",
    "        toDownload.append(index)\n",
    "        continue\n",
    "      \n",
    "      \n",
    "      \n",
    "      year = link['E'].split('-')[0]\n",
    "      month = link['E'].split('-')[1]\n",
    "      day = link['E'].split('-')[2][:2]\n",
    "      \n",
    "      # print(year, month, day)\n",
    "      \n",
    "      corresponding_file_name_begin = 'GMT' + year + month + day + '-'\n",
    "      corresponding_file_name_end = '.mp4'\n",
    "\n",
    "      file_exists = False\n",
    "      \n",
    "      actual_files = os.listdir(download_root + corresponding_folder_name)\n",
    "      for actual_file_name in actual_files:\n",
    "        if (actual_file_name.startswith(corresponding_file_name_begin) and actual_file_name.endswith(corresponding_file_name_end)):\n",
    "      \n",
    "              file_exists = True\n",
    "              break\n",
    "          \n",
    "      if (not file_exists):\n",
    "          toDownload.append(index)\n",
    "      data.append({'folder':corresponding_folder_name, 'file_name_begin': corresponding_file_name_begin})\n",
    "print('index',index)\n",
    "print('todownload', len(toDownload))\n",
    "print(toDownload)\n",
    "\n",
    "with open(download_root + 'progress_logs_v3.json','w') as file:\n",
    "  json.dump({'borrowed':[], 'to_download':toDownload}, file)\n",
    "\n",
    "print('---------------------------------------')\n",
    "print('successfully saved to_download indexes at progress_logs_v3.json')\n",
    "print('---------------------------------------')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actual code to download by link_indexes\n",
    "- code looks nasty with lots of un-necessary delays, but it works, so I'm not touching it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual code to download\n",
    "# stores code to file: 'app_v3.js'\n",
    "%%writefile ./drive/MyDrive/zoom_downloads/app_v3.js\n",
    "'use strict';\n",
    "\n",
    "// for colab code\n",
    "const linksPath = './drive/MyDrive/zoom_downloads/Fuse-Links.xlsx';\n",
    "const download_root = './drive/MyDrive/zoom_downloads/';  // create a folder called  'zoom_downloads' in MyDrive where downloads are to be stored\n",
    "\n",
    "// for running code locally\n",
    "// var download_root = '/home/gayatri/Documents/college/zoom_downloads/downloads/';\n",
    "// var linksPath = '/home/gayatri/Documents/college/zoom_downloads/Fuse-Links.xlsx';  // colab: './drive/MyDrive/zoom_downloads/Fuse-Links.xlsx'\n",
    "\n",
    "\n",
    "const excelToJson = require('convert-excel-to-json');\n",
    "// const puppeteer = require('puppeteer');\n",
    "const puppeteer = require('puppeteer-extra')\n",
    "// add stealth plugin and use defaults (all evasion techniques)\n",
    "const StealthPlugin = require('puppeteer-extra-plugin-stealth')\n",
    "puppeteer.use(StealthPlugin())\n",
    "\n",
    "var fs = require('fs'); // to create folder if not exist // reference: https://colab.research.google.com/drive/168X6Zo0Yk2fzEJ7WDfY9Q_0UOEmHSrZc?usp=sharing\n",
    "const { ConsoleMessage } = require('puppeteer')\n",
    "var progress_stored_previoiusly = 0;\n",
    "// var borrowed = new Array();     // borrow link to download so that no link is downloaded twice\n",
    "// var to_download = new Array();  // links to download\n",
    "\n",
    "// add recaptcha plugin and provide it your 2captcha token (= their apiKey)\n",
    "// 2captcha is the builtin solution provider but others would work as well.\n",
    "// Please note: You need to add funds to your 2captcha account for this to work\n",
    "const RecaptchaPlugin = require('puppeteer-extra-plugin-recaptcha')\n",
    "puppeteer.use(\n",
    "  RecaptchaPlugin({\n",
    "    provider: {\n",
    "      id: '2captcha',\n",
    "      token: '' // REPLACE THIS WITH YOUR OWN 2CAPTCHA API KEY âš¡\n",
    "    },\n",
    "    visualFeedback: true // colorize reCAPTCHAs (violet = detected, green = solved)\n",
    "  })\n",
    ")\n",
    "\n",
    "\n",
    "const delay = ms => new Promise(resolve => {\n",
    "    console.log(\"sleeping for \" + ms/1000 + \" s\");\n",
    "    setTimeout(resolve, ms);\n",
    "});\n",
    "\n",
    "// syncronous delay\n",
    "function delay_sync(ms) {\n",
    "  console.log(\"syncronous delay  \" + ms/1000 + \" s\");\n",
    "  return new Promise(resolve => setTimeout(resolve, ms));\n",
    "}\n",
    "\n",
    "// const randomInteger = require('random-int');\n",
    "const randomInteger = (min, max) => {\n",
    "  return Math.floor(Math.random() * (max - min + 1)) + min;\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "// create 'progress_logs_v2.json' if file doesnot exist\n",
    "if (!fs.existsSync(download_root + 'progress_logs_v2.json')){\n",
    "  console.log('\\ncreating file: progress_logs_v2.json...\\n');\n",
    "  fs.writeFileSync(download_root + 'progress_logs_v2.json', JSON.stringify(\n",
    "    {\n",
    "      \"comment\":\"progress for sheets are generated and updated based on sheet_name automatically ..\"\n",
    "    }\n",
    "  ));\n",
    " }\n",
    "\n",
    "// Create 'error_links_logs' if file doesnot exist\n",
    "if (!fs.existsSync(download_root + 'error_logs.json')){\n",
    "  console.log('\\ncreating file: error_logs.json...\\n');\n",
    "  fs.writeFileSync(download_root + 'error_logs.json', JSON.stringify(\n",
    "      {\n",
    "          link_logs:[],\n",
    "          other_logs:[],\n",
    "      }\n",
    "      ));\n",
    "}\n",
    "\n",
    "let load_json_data = (file_path) => {\n",
    "  // loading error links\n",
    "  // var saved_data;\n",
    "  try {\n",
    "      var saved_data = fs.readFileSync(file_path, 'utf-8');\n",
    "      saved_data = JSON.parse(saved_data);\n",
    "      // console.log('Loaded Links...: \\n ' + saved_data);\n",
    "      return saved_data;\n",
    "  } catch (error) {\n",
    "      console.log('Error Loading json file ...: \\n ' + file_path + error); \n",
    "  }\n",
    "}\n",
    "\n",
    "// load data test\n",
    "// data = {'borrowed':[0,1], 'to_download':[0,1,2,3,4,5,6,7,8,9]}\n",
    "var data = load_json_data(download_root + 'progress_logs_v3.json');\n",
    "var borrowed = [...data.borrowed]       // links download in progress\n",
    "console.log('\\nborrowed: ',borrowed)\n",
    "var to_download = [...data.to_download]    // ALL LINKS INITIALLY\n",
    "console.log('\\no_download', to_download)\n",
    "\n",
    "let save_json_data = function (data, file_path){\n",
    "  let to_save_data  = JSON.stringify(data);\n",
    "try {\n",
    "    fs.writeFileSync(file_path, to_save_data);\n",
    "} catch (error) {\n",
    "    console.log('Error saving data to file error_data...' + file_path + error);\n",
    "}\n",
    "}\n",
    "\n",
    "\n",
    "// update current progress of sheet after asynchronously waiting for download_time seconds. \n",
    "let async_wait_and_update_current_download_progress =  async (how_long_after_to_assume_downloaded, borrowed, to_download) => {\n",
    "  setTimeout(function(){\n",
    "    console.log('to_download:', to_download)\n",
    "    console.log('borrowed: ', borrowed)\n",
    "    save_json_data({'borrowed':[...borrowed], 'to_download':[...to_download]}, download_root + 'progress_logs_v2.json');\n",
    "    console.log('saved...')\n",
    "      // let current_date_ms = Date.parse(new Date());\n",
    "      // console.log(`updated download index to: ${current_link_index} for sheet: ${current_sheet}`);\n",
    "\n",
    "  }, how_long_after_to_assume_downloaded);//wait 2 seconds\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "let append_error_logs = (new_log, where) => {\n",
    "  // where = 'link_logs' or 'other_logs'\n",
    "\n",
    "  let previous_logs = load_json_data(download_root + 'error_logs.json');  // load previous logs\n",
    "  // console.log('previous_logs' + previous_logs);\n",
    "  // console.log('prev' + previous_logs);\n",
    "  previous_logs[where].push(new_log);                   // update logs\n",
    "  // console.log(previous_logs[where]);\n",
    "  \n",
    "  let initial_date = Date.now()\n",
    "  \n",
    "  while (Date.now() <= initial_date + 10000){}\n",
    "  save_json_data(previous_logs, download_root + 'error_logs.json');    // store error logs\n",
    "  previous_logs = JSON.stringify(previous_logs);\n",
    "  console.log('\\nappended error log ...\\n');\n",
    "  console.log('\\n waiting 15 minutes sync for remaining downloads to finis \\n')\n",
    "  // update links.indexOf\n",
    "  delay_sync(900000); // waiting 15 minutes for remaining downloads to finish\n",
    "}\n",
    "\n",
    "\n",
    "// js progress generator function\n",
    "function progress_bar(current_progress, total, label){\n",
    "  let progress = Math.round((current_progress/total)*100);\n",
    "  console.log(`${label}_progress  : ${progress}% :: `, current_progress, '/', total);\n",
    "  console.log(total);\n",
    "  let bar = [];\n",
    "  for (let i = 0; i < 100; i++){\n",
    "    if (i < progress){\n",
    "      bar.push('â–ˆ');\n",
    "    } else {\n",
    "      bar.push('â–‘');\n",
    "    }\n",
    "  }\n",
    "  // return bar.join(''); // array to string\n",
    "  console.log(bar.join(''));\n",
    "}\n",
    "// for (let i=0;i<10000;i++ ){progress_bar(i,10000, 'count')} # test progress_bar\n",
    "\n",
    "\n",
    "\n",
    "const download_links = async (links, download_root, current_sheet, progress_stored_previoiusly) => {\n",
    "  // initialize data progress logs v2\n",
    "    // to_download = [...Array(links.length).keys()]\n",
    "    // console.log('\\n\\nto download', to_download)\n",
    "    // borrowed = []\n",
    "    // save_json_data({'borrowed':[...borrowed], 'to_download':[...to_download]}, download_root + 'progress_logs_v2.json');\n",
    "    // return\n",
    "\n",
    "    \n",
    "  \n",
    "  \n",
    "  // initialize browser\n",
    "  // refrence: https://colab.research.google.com/drive/168X6Zo0Yk2fzEJ7WDfY9Q_0UOEmHSrZc?usp=sharing\n",
    "  // google cloud console\n",
    "  // const browser = await puppeteer.launch({executablePath:\"/opt/google/chrome/google-chrome\", args:['--no-sandbox','--start-maximized'], ignoreHTTPSErrors: true, headless: true});\n",
    "  // colab specific\n",
    "  const browser = await puppeteer.launch({executablePath:\"/usr/bin/chromium-browser\", args:['--no-sandbox','--start-maximized'], ignoreHTTPSErrors: true, headless: true});\n",
    "  // const browser = await puppeteer.launch({headless:false, ignoreHTTPSErrors: true}); // colab: const browser = await puppeteer.launch({executablePath:\"/usr/bin/chromium-browser\", args:['--no-sandbox','--start-maximized'], ignoreHTTPSErrors: true, headless: true});  // colab code:  \n",
    "  \n",
    "  const page = await browser.newPage();\n",
    "    await page.setExtraHTTPHeaders({\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "    });\n",
    "    await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36');\n",
    "    \n",
    "    // accept cookies\n",
    "    // console.log('opening https://zoom.us/ and accepting cookies ...');\n",
    "    // await page.goto('https://zoom.us/', {timeout: 60000});//, {waitUntil: 'networkidle0'});\n",
    "    // await page.screenshot({path: screenShotPath + 'before-accept-cookies' +'.png'});\n",
    "    // await page.waitForSelector('#onetrust-accept-btn-handler');\n",
    "    // document.querySelector('#onetrust-accept-btn-handler').click()\n",
    "    // await page.screenshot({path: screenShotPath + 'after-accept-cookies' +'.png'});\n",
    "    // console.log('accepted cookies ...')\n",
    "\n",
    "    // await delay(2000);  // wait 2 seconds\n",
    "\n",
    "    // file_name\n",
    "    // f'[count_index] + [{class_name[:170]}] + [{course_name}] + [{Grade}] + [{Duration}] + [{StartDateTime}] + [{EndDateTime}] + [{InstructorName}]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    // var dmPage = await browser.newPage()  // download manager page\n",
    "    // await dmPage.goto('chrome://downloads/')\n",
    "\n",
    "\n",
    "    \n",
    "      const screenShotPath = download_root + \"screenshots/\";\n",
    "      // for (let [current_link_index, link] of links.entries() ){\n",
    "      for(let current_link_index of to_download){\n",
    "        let link = links[parseInt(current_link_index)]\n",
    "        // data = load_json_data(download_root + 'progress_logs_v2.json');\n",
    "        // borrowed = [...data.borrowed]       // links download in progress\n",
    "        // console.log('\\nborrowed: ',borrowed)\n",
    "        // to_download = [...data.to_download]    // ALL LINKS INITIALLY\n",
    "        // console.log('\\no_download', to_download)\n",
    "\n",
    "        // if (current_link_index % 2 ==0 && current_link_index != 0) await delay(180000) // 5 MINUTES BREAK AFTER EVERY 5 DOWNLOADS\n",
    "        if(to_download.indexOf(current_link_index) == -1){\n",
    "          console.log(`\\n link: ${current_link_index} not in downloada skipping...\\n`)\n",
    "          continue\n",
    "        } else if (borrowed.indexOf(current_link_index) != -1){ // skip if already downloaded) {\n",
    "          console.log(`\\n link: ${current_link_index} in borrowed skipping...\\n`);\n",
    "          continue;\n",
    "          // skip if link is already in progress\n",
    "        } \n",
    "        \n",
    "        \n",
    "        console.log(`\\n link: ${current_link_index} not in borrowed downloading...\\n`)\n",
    "        borrowed.push(current_link_index); // add link to borrowed list\n",
    "        save_json_data({'borrowed':[...borrowed], 'to_download':[...to_download]}, download_root + 'progress_logs_v2.json');\n",
    "        \n",
    "        \n",
    "        progress_bar(current_link_index, links.length, 'download');  // displays progress of download\n",
    "        try {\n",
    "          \n",
    "          \n",
    "          let className = link['A'];\n",
    "          if (className == 'Class Name') continue; // skip title row\n",
    "          \n",
    "          let courseName = String(link['B']).replace(\"/\", \"|\");\n",
    "          let grade = String(link['C']).replace(\"/\", \"|\");\n",
    "          let duration = link['D'];\n",
    "          let startDateTime = link['E'];\n",
    "          let endDateTime = link['F'];\n",
    "          let instructorName = String(link['G']).replace(\"/\", \"|\");\n",
    "          let password = link['I'];\n",
    "          let url = link['J'];\n",
    "\n",
    "          if (url.slice(0,4) !='http') {\n",
    "            append_error_logs({courseName: courseName, grade: grade, duration: duration, startDateTime: startDateTime, endDateTime: endDateTime, instructorName: instructorName, password: password, url: url, error_value: 'url not link'},'other_logs');\n",
    "            continue; // skip non download url\n",
    "        }\n",
    "\n",
    "          // folders path for each download link\n",
    "          const rootDir = download_root;  // colab:  './drive/MyDrive/zoom_downloads/'\n",
    "          const subjectFolderName =  `${courseName}[${grade}][${instructorName}]`;\n",
    "          const downloadPath = rootDir + subjectFolderName;\n",
    "          console.log('\\ndownload_path: ', downloadPath, 'screenshot:' , screenShotPath, '\\n\\n')\n",
    "\n",
    "          // create folder if not exists\n",
    "          if (!fs.existsSync(downloadPath)){\n",
    "            fs.mkdirSync(downloadPath);\n",
    "            console.log(`creating folder: ${downloadPath}`);\n",
    "            \n",
    "            if (!fs.existsSync(screenShotPath)){    // create screenshot path if don't exist\n",
    "                fs.mkdirSync(screenShotPath);\n",
    "            }\n",
    "          }\n",
    "\n",
    "          // set download location \n",
    "          const client = await page.target().createCDPSession();\n",
    "          console.log(downloadPath);\n",
    "          await client.send('Page.setDownloadBehavior', {\n",
    "            behavior: 'allow',\n",
    "            downloadPath: downloadPath,\n",
    "            eventsEnabled: true,\n",
    "          })\n",
    "          \n",
    "          /*// test if download works\n",
    "          try{await page.goto('https://media.istockphoto.com/photos/emoji-holding-loudspeaker-isolated-on-white-background-picture-id961333268?p=1');\n",
    "            await delay(60000);// 60 sec delay\n",
    "        } catch(err){console.log(err);}*/\n",
    "            await page.goto(url, {timeout: 30000, waitUntil: 'networkidle2'});\n",
    "            await page.screenshot({path: download_root + 'screen_before_click_download.png'});\n",
    "\n",
    "            // await page.waitForSelector('#passcode');\n",
    "            try{\n",
    "              await page.type('#password', password);\n",
    "              await delay(randomInteger(1000, 5000)); // random delay betn 1 and 5 seconds\n",
    "              \n",
    "              // solving capatcha\n",
    "              // That's it, a single line of code to solve reCAPTCHAs ðŸŽ‰\n",
    "              // try{\n",
    "              //   await page.solveRecaptchas()\n",
    "              //   await Promise.all([\n",
    "              //     page.waitForNavigation(),\n",
    "              //     page.click(`#recaptcha-demo-submit`)\n",
    "              //   ])\n",
    "              // } catch(err){console.log(err)}\n",
    "              \n",
    "              // submit password\n",
    "              await page.click('.submit');\n",
    "            } catch(err) {\n",
    "              await page.type('#passcode', password);\n",
    "              await delay(randomInteger(1000, 5000)); // random delay betn 1 and 5 seconds\n",
    "              \n",
    "              // solving capatcha\n",
    "              // That's it, a single line of code to solve reCAPTCHAs ðŸŽ‰\n",
    "              // try{\n",
    "              //   await page.solveRecaptchas()\n",
    "              //   await Promise.all([\n",
    "              //     page.waitForNavigation(),\n",
    "              //     page.click(`#recaptcha-demo-submit`)\n",
    "              //   ])\n",
    "              // } catch(err){console.log(err)}\n",
    "              \n",
    "              // submit password\n",
    "              await page.click('#passcode_btn');\n",
    "\n",
    "\n",
    "            console.log(`downloading... sheet:${current_sheet} link_index:${current_link_index}` + String(url));\n",
    "            \n",
    "            // console.log(`current_link_index: ${current_link_index}`);\n",
    "            // await async_wait_and_update_current_download_progress(180000, parseInt(current_link_index) + parseInt(progress_stored_previoiusly), current_sheet); // assume downloaded after 3 minutes\n",
    "            \n",
    "            await async_wait_and_update_current_download_progress(35000, borrowed , to_download)\n",
    "            await delay(randomInteger(35000, 75000)); // random delay betn 35 and 65 seconds after each download click\n",
    "          \n",
    "            borrowed.splice(borrowed.indexOf(current_link_index), 1)  // remove link from borrowed\n",
    "            // console.log(borrowed)\n",
    "            to_download.splice(to_download.indexOf(current_link_index), 1)    // remove link from to_download\n",
    "            // console.log(to_download)\n",
    "            // save_json_data({'borrowed':[...borrowed], 'to_download':[...to_download]}, download_root + 'progress_logs_v2.json');\n",
    "            \n",
    "          \n",
    "          // screenshot before each delay :: 10 screenshots\n",
    "          await page.screenshot({path: screenShotPath + current_link_index +'.png'});\n",
    "          \n",
    "          \n",
    "          await page.screenshot({path: download_root + 'screen_after_click_download.png'});\n",
    "          \n",
    "          // display any error by zoom <zoom sometimes give '401 unauthorized' error >\n",
    "          let error_element = await page.$('#error_msg')\n",
    "          let error_value = await page.evaluate(el => el.textContent, error_element)\n",
    "          if (!(String(error_value) == '')){\n",
    "            // storing error log\n",
    "            append_error_logs({courseName: courseName, grade: grade, duration: duration, startDateTime: startDateTime, endDateTime: endDateTime, instructorName: instructorName, password: password, url: url, error_value: error_value}, 'link_logs');\n",
    "            \n",
    "            // displaying error message in console \n",
    "            console.log('Error\\n' + error_value);\n",
    "            console.log('On Link: ' + link);\n",
    "            // delay_sync(180000); // 3 minutes delay\n",
    "            break\n",
    "            // process.exit(\"Exit: this is the error of zoom (maybe wait few minutes and re-run the script)\");\n",
    "            //continue;\n",
    "          }\n",
    "\n",
    "        \n",
    "      } catch (error) {\n",
    "        // storing error log\n",
    "        append_error_logs({link:link, error_value: JSON.stringify(error)},'link_logs');\n",
    "\n",
    "        // displaying error message in console \n",
    "        console.log('\\n' + 'Error' + error + '\\n');\n",
    "        console.log('On Link: ' + link);\n",
    "        \n",
    "        console.log('Waiting 5 minutes to let pending downloads to finish');\n",
    "        delay_sync(300000); // 5 minutes delay\n",
    "        break\n",
    "      }  \n",
    "      }\n",
    "\n",
    "\n",
    "\n",
    "      // // wait till pending downloads are finished.\n",
    "      // // source: https://stackoverflow.com/a/69215213\n",
    "      // console.log('waiting for downoads to finish ...');\n",
    "      // await dmPage.bringToFront(); // this is necessary\n",
    "      // await dmPage.waitForFunction(\n",
    "      //     () => {\n",
    "      //         // monitoring the state of the first download item\n",
    "      //         // if finish than return true; if fail click\n",
    "      //         const total_downloads_count = document.querySelector('downloads-manager').shadowRoot.querySelector('#mainContainer').childElementCount;\n",
    "      //         all_downloaded = 1\n",
    "      //         for (let id_count =0;id_count < total_downloads_count; id_count ++){\n",
    "      //           if (document.querySelector('downloads-manager')[shadowRoot].querySelector(`#frb${id_count}`)[shadowRoot].querySelector('#content').classList.length > 1)\n",
    "      //           {\n",
    "      //             all_downloaded = 0\n",
    "      //             break;\n",
    "      //           }\n",
    "      //         }\n",
    "      //         if (all_downloaded == 1) return true;\n",
    "      //         console.log('all items not downloaded...');\n",
    "      //     },\n",
    "      //     { polling: 'raf', timeout: 0 }, // polling? yes. there is a 'polling: \"mutation\"' which kind of async\n",
    "      // );\n",
    "      // console.log('download finished, closing browser ...');\n",
    "      console.log('\\n\\n --------------- ************************* --------------- ');\n",
    "      console.log(' --- completed clicking download btn -> download in progress --- \\n ------------ waiting 10 minutes ------------ ');\n",
    "      console.log(' --------------- ************************* --------------- \\n\\n ');\n",
    "      delay_sync(180000);\n",
    "      process.exit(\"Exit: this is the error of zoom (maybe wait few minutes and re-run the script)\");\n",
    "    // await delay(randomInteger(720000, 1200000)); // random delay betn 12 and 20 minutes\n",
    "    // waiting 15 minutes before closing browser after clicking download to all links of specific sheet\n",
    "    console.log(`\\n Closing sheet: ${current_sheet} browser after waiting for 10 minutes after \\\"last link download click\\\" for download to complete. \\n`);\n",
    "    await browser.close();\n",
    "}\n",
    "\n",
    "\n",
    "// const auth_values = [{'url':'https://zoom.us/rec/download/KUl1-kXVTdIOH8ghAHDYVMXd1ZSf6eRcMnJ3m4gAZaUTPJryavrqub0ty8hzIiDGHbaWu02BCM5b-_bZ.tN6AJP4a9wIOcZJI', 'password':'Lt_j15r_'}];\n",
    "// download_it(auth_values)\n",
    "\n",
    "var all_sheets_links = excelToJson({  sourceFile: linksPath  });\n",
    "try {\n",
    "  // concatinate all the sheets\n",
    "  // var all_links = []\n",
    "    for (let current_sheet in all_sheets_links){ // loop through all sheets\n",
    "        console.log(current_sheet);\n",
    "        let sheet_links = all_sheets_links[current_sheet]\n",
    "        if (sheet_links == undefined) {console.log('Empty no data in the sheet'); break;} // break if sheet is empty\n",
    "        // console.log(sheet_links.slice(0,3));\n",
    "        // all_links = all_links.concat(links[current_sheet]);\n",
    "\n",
    "        // console.log('extracted links ... ' + String(sheet_links.slice(0,2)));\n",
    "        let download_progress = load_json_data(download_root + 'progress_logs.json');\n",
    "        \n",
    "        // check if progress for sheet exists\n",
    "        // and continue download from saved progress link_index\n",
    "        if (download_progress[current_sheet] != 0 && download_progress[current_sheet] != undefined ){\n",
    "          try {\n",
    "            // console.log(sheet_links);\n",
    "            console.log(`Sheet ${current_sheet} : (${sheet_links.length}) links loaded`);\n",
    "            // let len_all_links = sheet_links.length\n",
    "            progress_stored_previoiusly = parseInt(download_progress[current_sheet])\n",
    "            if (sheet_links == []) {\n",
    "              console.log(`\\n\\n All links from ${current_sheet} are downloaded!! \\n\\n`);\n",
    "              continue;\n",
    "            }\n",
    "          } catch (error) {\n",
    "            console.log(`Error slicing links for sheet: ${current_sheet} :: error:`  + error );\n",
    "          }\n",
    "        }\n",
    "        \n",
    "        // sheet_links= sheet_links.slice(0,2);\n",
    "        console.log('\\n\\n continuing from previous progress at index: ', progress_stored_previoiusly,'\\n\\n')\n",
    "        console.log(`Sheet ${current_sheet} : (${sheet_links.length}) links to be downloaded `);\n",
    "        download_links(sheet_links, download_root, current_sheet, progress_stored_previoiusly);\n",
    "  }\n",
    "} catch (error) {\n",
    "  // store error reading the link: link_path, error_message\n",
    "  append_error_logs({linksPath:linksPath, error_msg: 'error reading links file', error_value: String(error)}, 'other_logs');\n",
    "}\n",
    "\n",
    "\n",
    "/*\n",
    "- auto-find & download links not to download by folder_name & file_name\n",
    "- removed solveRecaptchas\n",
    "- auto-download all borrowed links :: simply clear borrowed list and re-run the script\n",
    "- zoom password submit showing two varients\n",
    "- zoom password input id: #passcode           previous: #password\n",
    "- zoom password submit  : #passcode_btn       previous: #submit\n",
    "- error message:          .zm-alert__content  previous: #error_msg\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for continue download in case of program termination by error\n",
    "# !node ./drive/MyDrive/zoom_downloads/not_downloaded.js\n",
    "%%writefile ./test.sh\n",
    "while true\n",
    "do \n",
    "  /tools/node/bin/node --trace-warnings ./drive/MyDrive/zoom_downloads/app_v3.js  # run this if colab stops download due to unverified capatcha\n",
    "  echo \"bash sleeping 120 seconds ...\"\n",
    "  sleep 210\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the loop\n",
    "!/bin/bash ./test.sh\n",
    "# !which node"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
