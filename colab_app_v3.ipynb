{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automatically downlod links not downloaded**<br>\n",
    "**or file that doesn't end with .mp4 extension**\n",
    "colab: https://colab.research.google.com/drive/1TCYgn7MPdptT_aDChtysmSuvhoiVb0nB#scrollTo=ZBwmLxMAISTd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installations\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# updating nodejs from v14 -> v16 :: reference : https://www.digitalocean.com/community/tutorials/how-to-install-node-js-on-debian-10\n",
    "# !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -\n",
    "# !sudo apt install nodejs\n",
    "# !node -v\n",
    "\n",
    "!apt-get update && upgrade\n",
    "!apt install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "!npm i convert-excel-to-json\n",
    "!npm install puppeteer\n",
    "!npm install puppeteer-extra\n",
    "!npm install puppeteer-extra --save\n",
    "!npm install puppeteer-extra-plugin-stealth --save   # installs plugin\n",
    "!npm install puppeteer-extra-plugin-recaptcha\n",
    "# !npm install -g npm to update # update npm\n",
    "# !npm install robots-txt-parser --save\n",
    "# !npm install random-int"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save links to json file cause it was hard to split link['E'], was converted to UTC and was showing err. link.E.getFullYear is not a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of all links to .json format\n",
    "# %%writefile ./drive/MyDrive/zoom_downloads/not_downloaded.js\n",
    "%%writefile ./test.js\n",
    "// check file that does not exist from links\n",
    "// may miss two classes same day\n",
    "\n",
    "var fs = require('fs');\n",
    "const excelToJson = require('convert-excel-to-json');\n",
    "const path = require('path');\n",
    "const download_root = './drive/MyDrive/zoom_downloads/'\n",
    "\n",
    "// function to save_json_data\n",
    "let save_json_data = function (data, file_path){\n",
    "  let to_save_data  = JSON.stringify(data);\n",
    "try {\n",
    "    fs.writeFileSync(file_path, to_save_data);\n",
    "} catch (error) {\n",
    "    console.log('Error saving data to file error_data...' + file_path + error);\n",
    "}\n",
    "}\n",
    "// function to load json data :: to test if data is actually saved\n",
    "let load_json_data = (file_path) => {\n",
    "  // loading error links\n",
    "  // var saved_data;\n",
    "  try {\n",
    "      var saved_data = fs.readFileSync(file_path, 'utf-8');\n",
    "      saved_data = JSON.parse(saved_data);\n",
    "      // console.log('Loaded Links...: \\n ' + saved_data);\n",
    "      return saved_data;\n",
    "  } catch (error) {\n",
    "      console.log('Error Loading json file ...: \\n ' + file_path + error); \n",
    "  }\n",
    "}\n",
    "var links_to_download = []\n",
    "const all_links = excelToJson({\n",
    "    sourceFile: download_root + 'Fuse-Links.xlsx'\n",
    "})['sagarmatha_live_class'];\n",
    "\n",
    "save_json_data(all_links, download_root + 'links.json')\n",
    "\n",
    "\n",
    "all_links_saved = load_json_data(download_root + 'links.json')\n",
    "console.log('all_links_saved', all_links_saved)\n",
    "if (all_links_saved) {\n",
    "  console.log('---------------------------------------')\n",
    "  console.log('successfully saved all links')\n",
    "  console.log('---------------------------------------')\n",
    "  }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!node test.js"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(python code)<br>\n",
    "saving link_indexes yet to be download at file: ./drive/MyDrive/zoom_downloads/progress_logs_v3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "download_root = './drive/MyDrive/zoom_downloads/'\n",
    "os.path.exists(download_root)\n",
    "\n",
    "\n",
    "toDownload = []\n",
    "toDownloadData = []\n",
    "# data = []\n",
    "with open(download_root + 'links.json','r') as file:\n",
    "  links = json.load(file)\n",
    "links = links[:]\n",
    "for index,link in enumerate(links[1:]):\n",
    "      if index == 1: continue  # pass first row (title row)\n",
    "      \n",
    "      link['B'] = link['B'].replace('/','|')\n",
    "      link['C'] = link['C'].replace('/','|')\n",
    "      link['G'] = link['G'].replace('/','|')\n",
    "      corresponding_folder_name = link['B'] + '[' + link['C'] + '][' + link['G'] + ']'\n",
    "\n",
    "      # file doesnot exists so need to downloads\n",
    "      if not os.path.exists(download_root + corresponding_folder_name):\n",
    "        print(f'folder doesn\\'t exist {corresponding_folder_name}')\n",
    "        toDownload.append(index)\n",
    "        toDownloadData.append({'url': link['J'], 'password':link['I'], 'path':corresponding_folder_name})\n",
    "        continue\n",
    "      \n",
    "      \n",
    "      \n",
    "      year = link['E'].split('-')[0]\n",
    "      month = link['E'].split('-')[1]\n",
    "      day = link['E'].split('-')[2][:2]\n",
    "      \n",
    "      # print(year, month, day)\n",
    "      \n",
    "      corresponding_file_name_begin = 'GMT' + year + month + day + '-'\n",
    "      corresponding_file_name_end = '.mp4'\n",
    "\n",
    "      file_exists = False\n",
    "      \n",
    "      actual_files = os.listdir(download_root + corresponding_folder_name)\n",
    "      for actual_file_name in actual_files:\n",
    "        if (actual_file_name.startswith(corresponding_file_name_begin) and actual_file_name.endswith(corresponding_file_name_end)):\n",
    "      \n",
    "              file_exists = True\n",
    "              break\n",
    "          \n",
    "      if (not file_exists):\n",
    "          toDownload.append(index)\n",
    "          toDownloadData.append({'url': link['J'], 'password':link['I'], 'path':corresponding_folder_name})\n",
    "      # data.append({'folder':corresponding_folder_name, 'file_name_begin': corresponding_file_name_begin})\n",
    "print('index',index)\n",
    "print('todownload', len(toDownload))\n",
    "print(toDownload)\n",
    "\n",
    "with open(download_root + 'progress_logs_v3.json','w') as file:\n",
    "  json.dump({'borrowed':[], 'to_download':toDownload}, file)\n",
    "\n",
    "with open(download_root + 'to_download_data_v3.json','w') as file:\n",
    "  json.dump({'data':toDownloadData}, file)\n",
    "\n",
    "print('---------------------------------------')\n",
    "print('successfully saved to_download indexes at progress_logs_v3.json')\n",
    "print('---------------------------------------')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actual code to download by link_indexes\n",
    "- code looks nasty with lots of un-necessary delays, but it works, so I'm not touching it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual code to download\n",
    "# stores code to file: 'app_v3.js'\n",
    "%%writefile ./drive/MyDrive/zoom_downloads/app_v3_1.js\n",
    "'use strict';\n",
    "\n",
    "// for colab code\n",
    "const linksPath = './drive/MyDrive/zoom_downloads/Fuse-Links.xlsx';\n",
    "const download_root = './drive/MyDrive/zoom_downloads/';  // create a folder called  'zoom_downloads' in MyDrive where downloads are to be stored\n",
    "\n",
    "// for running code locally\n",
    "// var download_root = '/home/gayatri/Documents/college/zoom_downloads/downloads/';\n",
    "// var linksPath = '/home/gayatri/Documents/college/zoom_downloads/Fuse-Links.xlsx';  // colab: './drive/MyDrive/zoom_downloads/Fuse-Links.xlsx'\n",
    "\n",
    "\n",
    "const excelToJson = require('convert-excel-to-json');\n",
    "// const puppeteer = require('puppeteer');\n",
    "const puppeteer = require('puppeteer-extra')\n",
    "// add stealth plugin and use defaults (all evasion techniques)\n",
    "const StealthPlugin = require('puppeteer-extra-plugin-stealth')\n",
    "puppeteer.use(StealthPlugin())\n",
    "\n",
    "var fs = require('fs'); // to create folder if not exist // reference: https://colab.research.google.com/drive/168X6Zo0Yk2fzEJ7WDfY9Q_0UOEmHSrZc?usp=sharing\n",
    "const { ConsoleMessage } = require('puppeteer')\n",
    "var progress_stored_previoiusly = 0;\n",
    "// var borrowed = new Array();     // borrow link to download so that no link is downloaded twice\n",
    "// var to_download = new Array();  // links to download\n",
    "\n",
    "// add recaptcha plugin and provide it your 2captcha token (= their apiKey)\n",
    "// 2captcha is the builtin solution provider but others would work as well.\n",
    "// Please note: You need to add funds to your 2captcha account for this to work\n",
    "const RecaptchaPlugin = require('puppeteer-extra-plugin-recaptcha')\n",
    "puppeteer.use(\n",
    "  RecaptchaPlugin({\n",
    "    provider: {\n",
    "      id: '2captcha',\n",
    "      token: '' // REPLACE THIS WITH YOUR OWN 2CAPTCHA API KEY âš¡\n",
    "    },\n",
    "    visualFeedback: true // colorize reCAPTCHAs (violet = detected, green = solved)\n",
    "  })\n",
    ")\n",
    "\n",
    "\n",
    "const delay = ms => new Promise(resolve => {\n",
    "    console.log(\"sleeping for \" + ms/1000 + \" s\");\n",
    "    setTimeout(resolve, ms);\n",
    "});\n",
    "\n",
    "// syncronous delay\n",
    "function delay_sync(ms) {\n",
    "  console.log(\"syncronous delay  \" + ms/1000 + \" s\");\n",
    "  return new Promise(resolve => setTimeout(resolve, ms));\n",
    "}\n",
    "\n",
    "// const randomInteger = require('random-int');\n",
    "const randomInteger = (min, max) => {\n",
    "  return Math.floor(Math.random() * (max - min + 1)) + min;\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "// create 'progress_logs_v3.json' if file doesnot exist\n",
    "if (!fs.existsSync(download_root + 'progress_logs_v3.json')){\n",
    "  console.log('\\ncreating file: progress_logs_v3.json...\\n');\n",
    "  fs.writeFileSync(download_root + 'progress_logs_v3.json', JSON.stringify(\n",
    "    {\n",
    "      \"comment\":\"progress for sheets are generated and updated based on sheet_name automatically ..\"\n",
    "    }\n",
    "  ));\n",
    " }\n",
    "\n",
    "// Create 'error_links_logs' if file doesnot exist\n",
    "if (!fs.existsSync(download_root + 'error_logs.json')){\n",
    "  console.log('\\ncreating file: error_logs.json...\\n');\n",
    "  fs.writeFileSync(download_root + 'error_logs.json', JSON.stringify(\n",
    "      {\n",
    "          link_logs:[],\n",
    "          other_logs:[],\n",
    "      }\n",
    "      ));\n",
    "}\n",
    "\n",
    "let load_json_data = (file_path) => {\n",
    "  // loading error links\n",
    "  // var saved_data;\n",
    "  try {\n",
    "      var saved_data = fs.readFileSync(file_path, 'utf-8');\n",
    "      saved_data = JSON.parse(saved_data);\n",
    "      // console.log('Loaded Links...: \\n ' + saved_data);\n",
    "      return saved_data;\n",
    "  } catch (error) {\n",
    "      console.log('Error Loading json file ...: \\n ' + file_path + error); \n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "let save_json_data = function (data, file_path){\n",
    "  let to_save_data  = JSON.stringify(data);\n",
    "try {\n",
    "    fs.writeFileSync(file_path, to_save_data);\n",
    "} catch (error) {\n",
    "    console.log('Error saving data to file error_data...' + file_path + error);\n",
    "}\n",
    "}\n",
    "\n",
    "\n",
    "// update current progress of sheet after asynchronously waiting for download_time seconds. \n",
    "let async_wait_and_update_current_download_progress =  async (how_long_after_to_assume_downloaded, borrowed, to_download) => {\n",
    "  setTimeout(function(){\n",
    "    console.log('to_download:', to_download)\n",
    "    console.log('borrowed: ', borrowed)\n",
    "    save_json_data({'borrowed':[...borrowed], 'to_download':[...to_download]}, download_root + 'progress_logs_v3.json');\n",
    "    console.log('saved...')\n",
    "      // let current_date_ms = Date.parse(new Date());\n",
    "      // console.log(`updated download index to: ${current_link_index} for sheet: ${current_sheet}`);\n",
    "\n",
    "  }, how_long_after_to_assume_downloaded);//wait 2 seconds\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "let append_error_logs = (new_log, where) => {\n",
    "  // where = 'link_logs' or 'other_logs'\n",
    "\n",
    "  let previous_logs = load_json_data(download_root + 'error_logs.json');  // load previous logs\n",
    "  // console.log('previous_logs' + previous_logs);\n",
    "  // console.log('prev' + previous_logs);\n",
    "  previous_logs[where].push(new_log);                   // update logs\n",
    "  // console.log(previous_logs[where]);\n",
    "  \n",
    "  let initial_date = Date.now()\n",
    "  \n",
    "  while (Date.now() <= initial_date + 10000){}\n",
    "  save_json_data(previous_logs, download_root + 'error_logs.json');    // store error logs\n",
    "  previous_logs = JSON.stringify(previous_logs);\n",
    "  console.log('\\nappended error log ...\\n');\n",
    "  console.log('\\n waiting 15 minutes sync for remaining downloads to finis \\n')\n",
    "  // update links.indexOf\n",
    "  delay_sync(900000); // waiting 15 minutes for remaining downloads to finish\n",
    "}\n",
    "\n",
    "\n",
    "// js progress generator function\n",
    "function progress_bar(current_progress, total, label){\n",
    "  let progress = Math.round((current_progress/total)*100);\n",
    "  console.log(`${label}_progress  : ${progress}% :: `, current_progress, '/', total);\n",
    "  console.log(total);\n",
    "  let bar = [];\n",
    "  for (let i = 0; i < 100; i++){\n",
    "    if (i < progress){\n",
    "      bar.push('â–ˆ');\n",
    "    } else {\n",
    "      bar.push('â–‘');\n",
    "    }\n",
    "  }\n",
    "  // return bar.join(''); // array to string\n",
    "  console.log(bar.join(''));\n",
    "}\n",
    "// for (let i=0;i<10000;i++ ){progress_bar(i,10000, 'count')} # test progress_bar\n",
    "\n",
    "\n",
    "\n",
    "const download_links = async (links) => {\n",
    "    // initialize browser\n",
    "    // refrence: https://colab.research.google.com/drive/168X6Zo0Yk2fzEJ7WDfY9Q_0UOEmHSrZc?usp=sharing\n",
    "    // google cloud console\n",
    "    // const browser = await puppeteer.launch({executablePath:\"/opt/google/chrome/google-chrome\", args:['--no-sandbox','--start-maximized'], ignoreHTTPSErrors: true, headless: true});\n",
    "    // colab specific\n",
    "    const browser = await puppeteer.launch({executablePath:\"/usr/bin/chromium-browser\", args:['--no-sandbox','--start-maximized'], ignoreHTTPSErrors: true, headless: true});\n",
    "    // const browser = await puppeteer.launch({headless:false, ignoreHTTPSErrors: true}); // colab: const browser = await puppeteer.launch({executablePath:\"/usr/bin/chromium-browser\", args:['--no-sandbox','--start-maximized'], ignoreHTTPSErrors: true, headless: true});  // colab code:  \n",
    "  \n",
    "    const page = await browser.newPage();\n",
    "    await page.setExtraHTTPHeaders({\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "    });\n",
    "    await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36');\n",
    "    \n",
    "    // accept cookies\n",
    "    // console.log('opening https://zoom.us/ and accepting cookies ...');\n",
    "    // await page.goto('https://zoom.us/', {timeout: 60000});//, {waitUntil: 'networkidle0'});\n",
    "    // await page.screenshot({path: screenShotPath + 'before-accept-cookies' +'.png'});\n",
    "    // await page.waitForSelector('#onetrust-accept-btn-handler');\n",
    "    // document.querySelector('#onetrust-accept-btn-handler').click()\n",
    "    // await page.screenshot({path: screenShotPath + 'after-accept-cookies' +'.png'});\n",
    "    // console.log('accepted cookies ...')\n",
    "\n",
    "    // await delay(2000);  // wait 2 seconds\n",
    "\n",
    "    // file_name\n",
    "    // f'[count_index] + [{class_name[:170]}] + [{course_name}] + [{Grade}] + [{Duration}] + [{StartDateTime}] + [{EndDateTime}] + [{InstructorName}]\n",
    "\n",
    "      const screenShotPath = download_root + \"screenshots/\";\n",
    "      // for (let [current_link_index, link] of links.entries() ){\n",
    "      //for(let current_link_index of to_download){\n",
    "      for (let link of links){\n",
    "        console.log('link', link);\n",
    "        // let link = links[parseInt(current_link_index)]\n",
    "        try {\n",
    "            let url = link['url']\n",
    "            let path = link['path']\n",
    "            let password = link['password']\n",
    "            let current_link_index = links.indexOf(link)\n",
    "            progress_bar(current_link_index, links.length, 'download');  // displays progress of download\n",
    "\n",
    "          if (url.slice(0,4) !='http') {\n",
    "            append_error_logs({courseName: courseName, grade: grade, duration: duration, startDateTime: startDateTime, endDateTime: endDateTime, instructorName: instructorName, password: password, url: url, error_value: 'url not link'},'other_logs');\n",
    "            continue; // skip non download url\n",
    "        }\n",
    "        const downloadPath = download_root + path;\n",
    "        console.log('point1');\n",
    "        // create folder if not exists\n",
    "        if (!fs.existsSync(downloadPath)){\n",
    "            fs.mkdirSync(downloadPath);\n",
    "            console.log(`creating folder: ${downloadPath}`);\n",
    "            \n",
    "            if (!fs.existsSync(screenShotPath)){    // create screenshot path if don't exist\n",
    "                fs.mkdirSync(screenShotPath);\n",
    "            }\n",
    "        }\n",
    "        console.log('point2');\n",
    "        // set download location \n",
    "        const client = await page.target().createCDPSession();\n",
    "        await client.send('Page.setDownloadBehavior', {\n",
    "            behavior: 'allow',\n",
    "            downloadPath: downloadPath,\n",
    "            eventsEnabled: true,\n",
    "        })  \n",
    "        console.log(`set download path: ${downloadPath}`)\n",
    "        console.log('point3');\n",
    "          \n",
    "        await page.goto(url, {timeout: 30000, waitUntil: 'networkidle2'});\n",
    "        await page.screenshot({path: download_root + 'screen_before_click_download.png'});\n",
    "        console.log('point4');\n",
    "        // await page.waitForSelector('#passcode');\n",
    "        try{\n",
    "              await page.type('#password', password);\n",
    "              await delay(randomInteger(1000, 5000)); // random delay betn 1 and 5 seconds\n",
    "              \n",
    "              console.log('point5');\n",
    "              // submit password\n",
    "              await page.click('.submit');\n",
    "            } catch(err) {\n",
    "              await page.type('#passcode', password);\n",
    "              await delay(randomInteger(1000, 5000)); // random delay betn 1 and 5 seconds\n",
    "              \n",
    "              // solving capatcha\n",
    "              // That's it, a single line of code to solve reCAPTCHAs ðŸŽ‰\n",
    "              // try{\n",
    "              //   await page.solveRecaptchas()\n",
    "              //   await Promise.all([\n",
    "              //     page.waitForNavigation(),\n",
    "              //     page.click(`#recaptcha-demo-submit`)\n",
    "              //   ])\n",
    "              // } catch(err){console.log(err)}\n",
    "              \n",
    "              // submit password\n",
    "              await page.click('#passcode_btn');\n",
    "              console.log(`downloading... link_index:${current_link_index}` + String(url));\n",
    "              await delay(randomInteger(35000, 80000)); // random delay betn 35 and 80 seconds after each download click  \n",
    "            }\n",
    "\n",
    "            \n",
    "          \n",
    "          // screenshot before each delay :: 10 screenshots\n",
    "          await page.screenshot({path: screenShotPath + current_link_index +'.png'});\n",
    "          \n",
    "          // display any error by zoom <zoom sometimes give '401 unauthorized' error >\n",
    "          // zoom displaying error in two ways sometimes first way, second_way other times\n",
    "          let error_element = await page.$('.zm-alert__content');\n",
    "          let second_error_element = await page.$('#error_msg');\n",
    "          if (error_element == null && second_error_element != null)error_element = second_error_element\n",
    "          \n",
    "          if (error_element != null){ \n",
    "            let error_value = await page.evaluate(el => el.textContent, error_element)\n",
    "            if (!(String(error_value) == '')){\n",
    "              // storing error log\n",
    "              append_error_logs({courseName: courseName, grade: grade, duration: duration, startDateTime: startDateTime, endDateTime: endDateTime, instructorName: instructorName, password: password, url: url, error_value: error_value}, 'link_logs');\n",
    "            \n",
    "              // displaying error message in console \n",
    "              console.log('Error\\n' + error_value);\n",
    "              console.log('On Link: ' + link);\n",
    "              // delay_sync(180000); // 3 minutes delay\n",
    "              break\n",
    "              // process.exit(\"Exit: this is the error of zoom (maybe wait few minutes and re-run the script)\");\n",
    "              //continue;\n",
    "            }\n",
    "          }\n",
    "\n",
    "        \n",
    "      } catch (error) {\n",
    "        // storing error log\n",
    "        append_error_logs({link:link, error_value: JSON.stringify(error)},'link_logs');\n",
    "\n",
    "        // displaying error message in console \n",
    "        console.log('\\n' + 'Error' + error + '\\n');\n",
    "        console.log('On Link: ' + link);\n",
    "        \n",
    "        console.log('Waiting 5 minutes to let pending downloads to finish');\n",
    "        delay_sync(300000); // 5 minutes delay\n",
    "        break\n",
    "      }  \n",
    "      }\n",
    "\n",
    "      console.log('\\n\\n --------------- ************************* --------------- ');\n",
    "      console.log(' --- completed clicking download btn -> download in progress --- \\n ------------ waiting 10 minutes ------------ ');\n",
    "      console.log(' --------------- ************************* --------------- \\n\\n ');\n",
    "      delay_sync(180000);\n",
    "      process.exit(\"Exit: this is the error of zoom (maybe wait few minutes and re-run the script)\");\n",
    "    // waiting 15 minutes before closing browser after clicking download to all links of specific sheet\n",
    "    console.log(`\\n Closing sheet: ${current_sheet} browser after waiting for 10 minutes after \\\"last link download click\\\" for download to complete. \\n`);\n",
    "    await browser.close();\n",
    "}\n",
    "\n",
    "let urls_path = download_root + \"to_download_data_v3.json\";\n",
    "var links_to_download = load_json_data(urls_path);\n",
    "links_to_download = links_to_download['data'];\n",
    "\n",
    "try{\n",
    "    download_links(links_to_download);\n",
    "} catch (error) {\n",
    "  // store error reading the link: link_path, error_message\n",
    "  append_error_logs({linksPath:urls_path, error_msg: 'error reading links file', error_value: String(error)}, 'other_logs');\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "/*\n",
    "- auto-find & download links not to download by folder_name & file_name\n",
    "- 'to_download_data_v3.json' stores url, path:<folder_name>, password\n",
    "- 'progress_logs_v3.json' stores index of links not downloaded :: not used anymore\n",
    "- removed solveRecaptchas\n",
    "- remove the concept of borrow\n",
    "- zoom password submit showing two varients\n",
    "- zoom password input id: #passcode           previous: #password\n",
    "- zoom password submit  : #passcode_btn       previous: #submit\n",
    "- error message:          .zm-alert__content  previous: #error_msg\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for continue download in case of program termination by error\n",
    "# !node ./drive/MyDrive/zoom_downloads/not_downloaded.js\n",
    "%%writefile ./test.sh\n",
    "while true\n",
    "do \n",
    "  /tools/node/bin/node --trace-warnings ./drive/MyDrive/zoom_downloads/app_v3.js  # run this if colab stops download due to unverified capatcha\n",
    "  echo \"bash sleeping 120 seconds ...\"\n",
    "  sleep 210\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the loop\n",
    "!/bin/bash ./test.sh\n",
    "# !which node"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
